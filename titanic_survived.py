# -*- coding: utf-8 -*-
"""titanic_생존률예측.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16LEvTGdr-ctUDlOc7RU47aQ8dzr7DE8b
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

titanic_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/머신러닝/자료/train.csv')
titanic_df.head(3)

print('## train 데이터 정보 ##')
print(titanic_df.info())

titanic_df['Age'].fillna(titanic_df['Age'].mean(),inplace = True)
# inplace = True : 기존 데이터프레임에 변경된 설정으로 덮어쓰겠다

titanic_df['Cabin'].fillna('N',inplace = True)
titanic_df['Embarked'].fillna('N',inplace = True)

print('데이터 세트 Null값 갯수',titanic_df.isnull().sum().sum()) #sum 하나 쓰면 속성별로 두개 쓰면 전부 합침 sum 안뜨면 true false로 뜸

print(' Sex 값 분포 :\n',titanic_df['Sex'].value_counts())

print('Cabin 값 분포 : \n',titanic_df['Cabin'].value_counts())

titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]
titanic_df['Cabin'].head(10)

print('Embarked 값 분포 : \n',titanic_df['Embarked'].value_counts())

titanic_df.groupby(['Sex','Survived'])['Survived'].count()

sns.barplot(x='Sex',y='Survived',data = titanic_df)

sns.barplot(x='Pclass',y='Survived',hue = 'Sex',data = titanic_df)
# sns.barplot(x='Pclass',y='Survived',data = titanic_df)

# 입력 age에 따라 구분값을 반환하는 함수 설정
def get_category(age):
  cat = ''
  if age <= -1: cat = 'Unknown'
  elif age <=5 : cat = 'Baby'
  elif age <=12 : cat = 'Child'
  elif age <=18 : cat = 'Teenager'
  elif age <=25 : cat = 'Student'
  elif age <=35 : cat = 'Young Adult'
  elif age <=60 : cat = 'Adult'
  else : cat = 'Elderly'
  return cat

#막대 그래프의 크기 figure를 더 크게 설정
plt.figure(figsize=(10,6))

#x축의 값을 순차적으로 표시하기 위한 설정
group_names = ['Unknown','Baby','Child','Teenager','Student','Young Adult','Adult','Elderly']

titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))
sns.barplot(x='Age_cat',y = 'Survived',hue = 'Sex',data = titanic_df,order = group_names)

titanic_df.drop('Age_cat',axis=1,inplace=True)

from sklearn import preprocessing
def encode_features(dataDF):
  features = ['Cabin','Sex','Embarked']
  for feature in features:
    le = preprocessing.LabelEncoder()
    le = le.fit(dataDF[feature])
    dataDF[feature] = le.transform(dataDF[feature])
  return dataDF

titanic_df = encode_features(titanic_df)
titanic_df.head(10)

# 데이터 분리(타식과 필요내용)
y_titanic_df = titanic_df['Survived']

X_titanic_df = titanic_df.drop(['PassengerId','Survived','Name','Ticket'],axis = 1)

X_titanic_df.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_titanic_df,y_titanic_df)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression 
from sklearn.metrics import accuracy_score

dt_clf = DecisionTreeClassifier()
rf_clf = RandomForestClassifier()
lr_clf = LogisticRegression()

dt_clf.fit(X_train,y_train)
dt_pred = dt_clf.predict(X_test)

df_survive = X_test
df_survive['Survive'] = dt_pred
print(df_survive)
print("DecisionTresClassifier 정확도: ",accuracy_score(y_test,dt_pred))

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_titanic_df,y_titanic_df)

rf_clf.fit(X_train,y_train)
rf_pred = rf_clf.predict(X_test)
df_survive = X_test
df_survive['Survive'] = rf_pred
print(df_survive)
print("RandomForestClassifier() 정확도: {0:.4f}".format(accuracy_score(y_test,rf_pred)))

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_titanic_df,y_titanic_df)

lr_clf.fit(X_train,y_train)
lr_pred = lr_clf.predict(X_test)
df_survive = X_test
df_survive['Survive'] = lr_pred
print(df_survive)
print("DecisionTresClassifier 정확도: {0:.4f}".format(accuracy_score(y_test,lr_pred)))

from sklearn.model_selection import KFold

def exec_kfold(clf,folds=5):
  #폴드 세트를 5개인 KFold 객체를 생성, 폴드 수만큼 예측결과 저장을 위한 리스트 객체 생성.
  kfold = KFold(n_splits=folds)
  scores = []

  #KFold교차 검증 수행
  for iter_count , (train_index,test_index) in enumerate(kfold.split(X_titanic_df)):
    # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가르키는 index 생성
    X_train,X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]
    y_train,y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]

    #Classsifier 학습, 예측, 정확도 계산
    clf.fit(X_train,y_train)
    predictions = clf.predict(X_test)
    accuracy = accuracy_score(y_test,predictions)
    scores.append(accuracy)
    print("교차 검증 {0} 정확도 : {1:.4f}".format(iter_count,accuracy))
    pass
  mean_score = np.mean(scores)
  print("평균 정확도: {0:.4f}".format(mean_score))

exec_kfold(dt_clf,folds=7)

from sklearn.model_selection import cross_val_score
scores = cross_val_score(dt_clf, X_titanic_df,y_titanic_df,cv=5)
for iter_count , accuracy in enumerate(scores):
  print("교차 검증 {0} 정확도 : {1:.4f}".format(iter_count,accuracy))
print("평균 정확도: {0:.4f}".format(np.mean(scores)))

from sklearn.model_selection import GridSearchCV

parameters = {'max_depth':[2,3,5,10],
             'min_samples_split':[2,3,5],'min_samples_leaf':[1,5,8]}

grid_dclf = GridSearchCV(dt_clf, param_grid = parameters, scoring='accuracy', cv=5)
grid_dclf.fit(X_train,y_train)

print('GridSearchCV 최적 하이퍼 파라미터: ',grid_dclf.best_params_)
print('GridSearchCV 최고 정확도 : {:.4f}'.format(grid_dclf.best_score_))

X_train,X_test,y_train,y_test = train_test_split(X_titanic_df,y_titanic_df)

best_dclf = grid_dclf.best_estimator_
# 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행
dpredictions = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test,dpredictions)
print('테스트 세트에서의 DecisionTreeClassifier 정확도 : {:.4f}'.format(accuracy))

